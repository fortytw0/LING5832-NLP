{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = \"data/en.train.json\"\n",
    "validation_data_path = \"data/en.dev.json\"\n",
    "test_data_path = \"data/en.trial.complete.json\"\n",
    "\n",
    "assert os.path.exists(train_data_path)\n",
    "assert os.path.exists(validation_data_path)\n",
    "assert os.path.exists(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(train_data_path) as f : \n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(validation_data_path) as f : \n",
    "    validation_data = json.load(f)\n",
    "    \n",
    "with open(test_data_path) as f : \n",
    "    test_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train Data Samples :  43608\n",
      "Total Val Data Samples :  6375\n",
      "Total Test Data Samples :  200\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Train Data Samples : \" , len(train_data))\n",
    "print(\"Total Val Data Samples : \" , len(validation_data))\n",
    "print(\"Total Test Data Samples : \" , len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checks...\n",
      "\n",
      "Sample Train Data : \n",
      "Available keys :  dict_keys(['id', 'gloss', 'sgns', 'char', 'electra'])\n",
      "ID :  en.train.1\n",
      "GLOSS :  A blemish .\n",
      "\n",
      "Sample Train Data : \n",
      "Available keys :  dict_keys(['id', 'gloss', 'sgns', 'char', 'electra'])\n",
      "ID :  en.dev.1\n",
      "GLOSS :  A meal consisting of food normally eaten in the morning , which may typically include eggs , sausages , toast , bacon , etc .\n",
      "\n",
      "Sample Train Data : \n",
      "Available keys :  dict_keys(['id', 'word', 'pos', 'gloss', 'example', 'type', 'counts', 'f_rnk', 'concrete', 'polysemous', 'sgns', 'char', 'electra'])\n",
      "ID :  en.trial.1\n",
      "GLOSS :  Pleasant ; clear .\n",
      "WORD :  beautiful\n",
      "POS :  adjective\n",
      "EXAMPLE :  It 's beautiful outside , let 's go for a walk .\n",
      "TYPE :  synonym/antonym-based\n",
      "COUNTS :  124908\n",
      "F_RNK :  706\n",
      "CONCRETE :  0\n",
      "POLYSEMOUS :  0\n",
      "\n",
      "Embedding sizes : \n",
      "SGNS SIZE :  256\n",
      "CHAR SIZE :  256\n",
      "ELECTRA SIZE :  256\n",
      "\n",
      "Maximum gloss length : \n",
      "Character level :  643\n",
      "Word level :  129\n"
     ]
    }
   ],
   "source": [
    "print('Sanity Checks...')\n",
    "print('\\nSample Train Data : ')\n",
    "print('Available keys : ' , train_data[0].keys())\n",
    "print('ID : ' , train_data[0]['id'])\n",
    "print('GLOSS : ' , train_data[0]['gloss'])\n",
    "\n",
    "\n",
    "print('\\nSample Train Data : ')\n",
    "print('Available keys : ' , validation_data[0].keys())\n",
    "print('ID : ' , validation_data[0]['id'])\n",
    "print('GLOSS : ' , validation_data[0]['gloss'])\n",
    "\n",
    "print('\\nSample Train Data : ')\n",
    "print('Available keys : ' , test_data[0].keys())\n",
    "print('ID : ' , test_data[0]['id'])\n",
    "print('GLOSS : ' , test_data[0]['gloss'])\n",
    "print('WORD : ' , test_data[0]['word'])\n",
    "print('POS : ' , test_data[0]['pos'])\n",
    "print('EXAMPLE : ' , test_data[0]['example'])\n",
    "print('TYPE : ' , test_data[0]['type'])\n",
    "print('COUNTS : ' , test_data[0]['counts'])\n",
    "print('F_RNK : ' , test_data[0]['f_rnk'])\n",
    "print('CONCRETE : ' , test_data[0]['concrete'])\n",
    "print('POLYSEMOUS : ' , test_data[0]['polysemous'])\n",
    "\n",
    "print('\\nEmbedding sizes : ')\n",
    "print('SGNS SIZE : ', len(test_data[0]['sgns']))\n",
    "print('CHAR SIZE : ', len(test_data[0]['char']))\n",
    "print('ELECTRA SIZE : ', len(test_data[0]['electra']))\n",
    "\n",
    "print('\\nMaximum gloss length : ')\n",
    "\n",
    "char_level = max(train_data+validation_data+test_data, key=lambda x: len(x['gloss']))\n",
    "word_level = max(train_data+validation_data+test_data, key=lambda x: len(x['gloss'].split()))\n",
    "\n",
    "print('Character level : ', len(char_level['gloss']))\n",
    "print('Word level : ', len(word_level['gloss'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing gloss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_gloss(gloss) : \n",
    "    \n",
    "    '''\n",
    "    1. Removes puntuations and numerals from the gloss. \n",
    "    2. Lowercases the words. \n",
    "    3. Returns list of words. \n",
    "    '''\n",
    "    \n",
    "    processed = re.sub(r'[^\\w\\s]','',gloss)\n",
    "    processed = processed.lower()\n",
    "    return processed.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_word_tokenizer(data, unk_ratio=0.0) : \n",
    "    \n",
    "    '''\n",
    "    Returns list of all words in the gloss. Index serves as the \n",
    "    tokenization index. \n",
    "    \n",
    "    unk_ratio is used to set a percentage of singletons in \n",
    "    the gloss to \"UNK\" token. \n",
    "    '''\n",
    "    \n",
    "    assert unk_ratio <= 1.00, 'unk_ratio should be between 0 and 1.'\n",
    "    tokens = []\n",
    "    token_counts = {}\n",
    "    \n",
    "    # preparing token list\n",
    "    \n",
    "    for word in test_data + train_data + validation_data : \n",
    "        processed_gloss = process_gloss(word['gloss'])\n",
    "\n",
    "        for token in processed_gloss :\n",
    "\n",
    "            if token not in tokens : \n",
    "                tokens.append(token)\n",
    "                token_counts[token] = 1\n",
    "            else : \n",
    "                token_counts[token] += 1\n",
    "                \n",
    "    \n",
    "    if unk_ratio > 0.0 :\n",
    "         \n",
    "        '''\n",
    "        If UNK tokens are required - \n",
    "        1. Find all the singleton tokens\n",
    "        2. unk_ratio of all the singleton tokens are bucketed in unk_tokens \n",
    "        3. Remove these unk_tokens from the tokens and token_counts \n",
    "        '''\n",
    "        \n",
    "        singleton_tokens = [token for token, count in token_counts.items() if count==1]\n",
    "        \n",
    "        num_unk_tokens = int(len(singleton_tokens) * unk_ratio)\n",
    "        unk_tokens = singleton_tokens[:num_unk_tokens]\n",
    "        \n",
    "        unk_token_count = 0\n",
    "        for token in unk_tokens : unk_token_count += token_counts[token] \n",
    "        \n",
    "        tokens = [token for token in tokens if token not in unk_tokens]\n",
    "        token_counts = {token:count for token, count in token_counts.items() if token not in unk_tokens}\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    1. Adding UNK token\n",
    "    2. Adding PAD token\n",
    "    '''\n",
    "    tokens.append(\"UNK\")\n",
    "    token_counts[\"UNK\"] = unk_token_count\n",
    "    tokens.append(\"PAD\")\n",
    "        \n",
    "    return tokens, token_counts\n",
    "\n",
    "def get_char_tokenizer(data) : \n",
    "    \n",
    "    tokens = list(string.ascii_lowercase + \" \" + \".\")\n",
    "    tokens.append(\"UNK\")\n",
    "    tokens.append(\"PAD\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = train_data + validation_data + test_data\n",
    "\n",
    "word_tokenizer, word_counts = get_word_tokenizer(data, unk_ratio=0.25)\n",
    "char_tokenizer = get_char_tokenizer(data)\n",
    "\n",
    "word_vocab_size = len(word_tokenizer)\n",
    "char_vocab_size = len(char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(gloss, tokenizer, tokenize_level=\"WORD\", pad=False, target_len=100) : \n",
    "    \n",
    "    processed_gloss = process_gloss(gloss)\n",
    "    tokens = []\n",
    "    \n",
    "    if tokenize_level==\"WORD\" :       \n",
    "        words = processed_gloss\n",
    "    else : \n",
    "        words = \" \".join(processed_gloss)\n",
    "        \n",
    "        \n",
    "    for word in words : \n",
    "        try : \n",
    "            tokens.append(tokenizer.index(word))\n",
    "        except : \n",
    "            tokens.append(tokenizer.index(\"UNK\"))\n",
    "    \n",
    "    if pad : \n",
    "        \n",
    "        pad_token = tokenizer.index(\"PAD\")\n",
    "        \n",
    "        if len(tokens)>=target_len : \n",
    "            return tokens[:target_len]\n",
    "        \n",
    "        while len(tokens) < target_len : \n",
    "            tokens.append(pad_token)\n",
    "    \n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Level Tokenizer : \n",
      "Token0 is UNK :  [25162, 3545, 25, 42, 9361]\n",
      "Token1 is UNK :  [18605, 25162, 18048, 42, 25162]\n",
      "Token0 is UNK :  [25162]\n",
      "\n",
      "Char Level Tokenizer : \n",
      "Padded :  [0, 17, 19, 7, 20, 17, 26, 11, 0, 24, 26, 8, 13, 26, 19, 7, 4, 26, 12, 20, 3, 29, 29, 29, 29, 29, 29, 29, 29, 29]\n",
      "Unpadded :  [5, 14, 17, 3, 26, 15, 17, 4, 5, 4, 2, 19, 26, 18, 0, 22, 26, 19, 7, 4, 26, 21, 14, 6, 14, 13, 18]\n",
      "Token13 is UNK :  [18, 11, 0, 17, 19, 8, 1, 0, 0, 17, 5, 0, 18, 28, 19]\n"
     ]
    }
   ],
   "source": [
    "print('Word Level Tokenizer : ')\n",
    "print('Token0 is UNK : ' , tokenize(\"arthur lay in the mud\", word_tokenizer, tokenize_level=\"WORD\"))\n",
    "print('Token1 is UNK : ' , tokenize(\"ford prefect saw the vogons\", word_tokenizer, tokenize_level=\"WORD\"))\n",
    "print('Token0 is UNK : ' , tokenize(\"slartibaarfast\", word_tokenizer, tokenize_level=\"WORD\"))\n",
    "print('\\nChar Level Tokenizer : ')\n",
    "print('Padded : ' , tokenize(\"arthur lay in the mud\", char_tokenizer, tokenize_level=\"CHAR\", pad=True, target_len=30))\n",
    "print('Unpadded : ' , tokenize(\"ford prefect saw the vogons\", char_tokenizer, tokenize_level=\"CHAR\"))\n",
    "print('Token13 is UNK : ' , tokenize(\"slartibaarfasΩt\", char_tokenizer, tokenize_level=\"CHAR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_generator(batch_size) :\n",
    "    \n",
    "    start = 0 \n",
    "    train_data_size = len(train_data)\n",
    "\n",
    "    while True : \n",
    "        \n",
    "        char_level_tokens = []\n",
    "        word_level_tokens = []\n",
    "\n",
    "        sgns_embeddings = []\n",
    "        char_embeddings = []\n",
    "        electra_embeddings = []\n",
    "        \n",
    "        end = start + batch_size\n",
    "        \n",
    "        if end > train_data_size : \n",
    "            end = train_data_size \n",
    "        elif end == train_data_size : \n",
    "            start = 0\n",
    "            end = start + batch_size\n",
    "        else : \n",
    "            pass\n",
    "        \n",
    "        curr_batch = train_data[start:end]\n",
    "        \n",
    "        for sample in curr_batch : \n",
    "            \n",
    "            gloss = sample['gloss']\n",
    "            \n",
    "            char_level_tokens.append(tokenize(gloss, char_tokenizer, tokenize_level=\"CHAR\", pad=True, target_len=char_seq_len))\n",
    "            word_level_tokens.append(tokenize(gloss, word_tokenizer, tokenize_level=\"WORD\", pad=True, target_len=word_seq_len))\n",
    "            \n",
    "            sgns_embeddings.append(sample['sgns'])\n",
    "            char_embeddings.append(sample['char'])\n",
    "            electra_embeddings.append(sample['electra'])\n",
    "            \n",
    "        yield (np.array(char_level_tokens), np.array(word_level_tokens)),\\\n",
    "        (np.array(sgns_embeddings), np.array(char_embeddings), np.array(electra_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, BatchNormalization, Embedding, Concatenate, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras import optimizers, losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "char_seq_len = 640\n",
    "word_seq_len = 128\n",
    "embedding_size = 256\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "steps_per_epoch = int(len(train_data)/epochs)\n",
    "batch_size = 256\n",
    "\n",
    "checkpoint_dir = 'models/vanilla_lstm2.0'\n",
    "csv_log_path = 'logs/vanilla_lstm2.0_training.log'\n",
    "history_log_path = 'logs/vanilla_lstm2.0_history.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(char_level_tokens, word_level_tokens), (sgns_embeddings, char_embeddings, electra_embeddings) = next(train_generator(batch_size))\n",
    "\n",
    "assert char_level_tokens.shape == (batch_size, char_seq_len)\n",
    "assert word_level_tokens.shape == (batch_size, word_seq_len)\n",
    "assert sgns_embeddings.shape == (batch_size, embedding_size)\n",
    "assert char_embeddings.shape == (batch_size, embedding_size)\n",
    "assert electra_embeddings.shape == (batch_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 14:39:14.138757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.149483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.150272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.151593: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 14:39:14.152155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.153127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.153950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.583112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.583974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.584797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-06 14:39:14.585565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "char_inputs = Input(shape=(char_seq_len, ))\n",
    "char_embedding = Embedding(char_vocab_size, embedding_size)(char_inputs)\n",
    "char_lstm = LSTM(128, recurrent_dropout=0.1, return_sequences=True)(char_embedding)\n",
    "char_lstm = LSTM(128, recurrent_dropout=0.1, return_sequences=True)(char_lstm)\n",
    "char_output = Flatten()(char_lstm)\n",
    "\n",
    "\n",
    "word_inputs = Input(shape=(word_seq_len, ))\n",
    "word_embedding = Embedding(word_vocab_size, embedding_size)(word_inputs)\n",
    "word_lstm = LSTM(128, recurrent_dropout=0.1, return_sequences=True)(word_embedding)\n",
    "word_lstm = LSTM(128, recurrent_dropout=0.1, return_sequences=True)(word_lstm)\n",
    "word_output = Flatten()(word_lstm)\n",
    "\n",
    "concatenated = Concatenate(axis=-1)([char_output, word_output])\n",
    "normalized = BatchNormalization()(concatenated)\n",
    "\n",
    "dense = Dense(2048, activation='relu')(normalized)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = Dense(1024, activation='relu')(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "dense = Dense(512, activation='relu')(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = Dense(512, activation='relu')(dense)\n",
    "dense = Dense(256, activation='relu')(dense)\n",
    "\n",
    "sgns_out = Dense(embedding_size, activation='tanh')(dense)\n",
    "chars_out = Dense(embedding_size, activation='tanh')(dense)\n",
    "electra_out = Dense(embedding_size, activation='tanh')(dense)\n",
    "\n",
    "\n",
    "model = Model(inputs=(char_inputs, word_inputs), \n",
    "             outputs=(sgns_out, chars_out, electra_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(), \n",
    "              loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(checkpoint_dir) : \n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_dir, save_freq=5*steps_per_epoch)\n",
    "\n",
    "csv_log_path = CSVLogger(csv_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 14:40:16.311287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "43/43 [==============================] - 246s 5s/step - loss: 4.4357 - dense_5_loss: 1.0030 - dense_6_loss: 0.2786 - dense_7_loss: 3.1541\n",
      "Epoch 2/1000\n",
      "43/43 [==============================] - 236s 5s/step - loss: 4.1828 - dense_5_loss: 0.8950 - dense_6_loss: 0.2263 - dense_7_loss: 3.0615\n",
      "Epoch 3/1000\n",
      "43/43 [==============================] - 235s 5s/step - loss: 4.0266 - dense_5_loss: 0.7629 - dense_6_loss: 0.2191 - dense_7_loss: 3.0446\n",
      "Epoch 4/1000\n",
      "43/43 [==============================] - 235s 5s/step - loss: 3.8724 - dense_5_loss: 0.6438 - dense_6_loss: 0.2090 - dense_7_loss: 3.0197\n",
      "Epoch 5/1000\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.7439 - dense_5_loss: 0.5517 - dense_6_loss: 0.1974 - dense_7_loss: 2.9947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:00:03.562640: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-12-06 15:00:10.708990: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2021-12-06 15:00:11.019468: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2021-12-06 15:00:11.331179: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vanilla_lstm2.0/assets\n",
      "43/43 [==============================] - 253s 6s/step - loss: 3.7439 - dense_5_loss: 0.5517 - dense_6_loss: 0.1974 - dense_7_loss: 2.9947\n",
      "Epoch 6/1000\n",
      "43/43 [==============================] - 225s 5s/step - loss: 3.6464 - dense_5_loss: 0.4879 - dense_6_loss: 0.1849 - dense_7_loss: 2.9736\n",
      "Epoch 7/1000\n",
      "43/43 [==============================] - 223s 5s/step - loss: 3.5753 - dense_5_loss: 0.4448 - dense_6_loss: 0.1745 - dense_7_loss: 2.9560\n",
      "Epoch 8/1000\n",
      "43/43 [==============================] - 221s 5s/step - loss: 3.5262 - dense_5_loss: 0.4157 - dense_6_loss: 0.1666 - dense_7_loss: 2.9439\n",
      "Epoch 9/1000\n",
      "43/43 [==============================] - 222s 5s/step - loss: 3.4871 - dense_5_loss: 0.3947 - dense_6_loss: 0.1593 - dense_7_loss: 2.9331\n",
      "Epoch 10/1000\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.4558 - dense_5_loss: 0.3788 - dense_6_loss: 0.1537 - dense_7_loss: 2.9233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:19:05.125287: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2021-12-06 15:19:05.437572: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vanilla_lstm2.0/assets\n",
      "43/43 [==============================] - 256s 6s/step - loss: 3.4558 - dense_5_loss: 0.3788 - dense_6_loss: 0.1537 - dense_7_loss: 2.9233\n",
      "Epoch 11/1000\n",
      "43/43 [==============================] - 219s 5s/step - loss: 3.4329 - dense_5_loss: 0.3669 - dense_6_loss: 0.1492 - dense_7_loss: 2.9167\n",
      "Epoch 12/1000\n",
      "43/43 [==============================] - 219s 5s/step - loss: 3.4142 - dense_5_loss: 0.3578 - dense_6_loss: 0.1453 - dense_7_loss: 2.9111\n",
      "Epoch 13/1000\n",
      "43/43 [==============================] - 224s 5s/step - loss: 3.3983 - dense_5_loss: 0.3501 - dense_6_loss: 0.1418 - dense_7_loss: 2.9064\n",
      "Epoch 14/1000\n",
      "43/43 [==============================] - 219s 5s/step - loss: 3.3855 - dense_5_loss: 0.3439 - dense_6_loss: 0.1391 - dense_7_loss: 2.9024\n",
      "Epoch 15/1000\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.3750 - dense_5_loss: 0.3388 - dense_6_loss: 0.1371 - dense_7_loss: 2.8991"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator(batch_size), \n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs, \n",
    "                   steps_per_epoch=steps_per_epoch, \n",
    "                   callbacks=[model_checkpoint, csv_log_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12473/1307766888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[-5.49655378e-01,  2.97194690e-01,  6.95654303e-02,\n",
       "         -8.11954066e-02,  5.77607214e-01, -3.92344892e-01,\n",
       "          1.21077761e-01, -1.22862101e-01,  8.06132674e-01,\n",
       "          4.18311864e-01, -3.53744537e-01, -7.54541218e-01,\n",
       "          3.82973224e-01, -3.80358666e-01,  3.59223634e-01,\n",
       "          5.42707980e-01,  7.37505034e-02,  3.34000178e-02,\n",
       "         -4.82165277e-01, -1.91515803e-01,  2.92198751e-02,\n",
       "         -5.04274905e-01,  2.36272603e-01, -1.15609974e-01,\n",
       "         -4.06451553e-01,  5.23873381e-02,  2.89239585e-01,\n",
       "          1.40692845e-01,  1.59932554e-01,  3.68848979e-01,\n",
       "         -2.48135790e-01,  4.65482831e-01, -1.03306912e-01,\n",
       "          7.01117814e-01,  3.12040746e-01, -6.93565488e-01,\n",
       "         -9.64240968e-01,  5.19052386e-01, -8.97340775e-01,\n",
       "          6.09122753e-01,  4.48237568e-01,  1.94943532e-01,\n",
       "         -1.02290325e-01, -1.53341323e-01, -4.47221808e-02,\n",
       "          5.21485172e-02,  2.69740939e-01, -2.36852556e-01,\n",
       "          7.38828769e-03,  2.25389019e-01,  1.41957328e-01,\n",
       "         -3.38339686e-01, -2.81663937e-03, -4.19736743e-01,\n",
       "         -3.33072245e-01,  1.15683660e-01,  2.90465534e-01,\n",
       "          3.27418923e-01,  1.22763380e-01,  1.61036775e-02,\n",
       "         -9.59120095e-01, -2.51113892e-01, -1.05884269e-01,\n",
       "         -6.27337098e-01,  1.94886163e-01,  7.60778487e-01,\n",
       "         -1.77907661e-01, -3.69365394e-01,  2.91818589e-01,\n",
       "          3.34672302e-01, -5.37802419e-03,  6.60045683e-01,\n",
       "          5.76162696e-01,  4.56544191e-01, -8.75301182e-01,\n",
       "         -3.42586339e-01, -2.77087629e-01,  2.22436693e-02,\n",
       "          4.24517274e-01, -9.53314528e-02, -3.09717625e-01,\n",
       "          6.87000036e-01,  1.73067480e-01,  1.97936758e-01,\n",
       "         -2.33444020e-01, -4.37566370e-01, -2.19087481e-01,\n",
       "         -8.90876949e-02, -6.92036748e-01,  6.06128156e-01,\n",
       "         -3.58151972e-01, -1.22364378e-02,  2.64905095e-01,\n",
       "         -1.76121458e-01,  1.62312672e-01,  1.93735197e-01,\n",
       "         -1.15675695e-01,  2.13025168e-01, -9.54995677e-02,\n",
       "          5.23718834e-01, -9.84418541e-02,  3.51538956e-01,\n",
       "          9.73265246e-02, -2.16760293e-01, -2.43305247e-02,\n",
       "          9.13977146e-01,  1.94902029e-02, -3.22893202e-01,\n",
       "         -9.73058566e-02, -3.68915737e-01,  3.37294966e-01,\n",
       "          3.10193636e-02, -4.50816005e-01, -1.72874048e-01,\n",
       "         -1.37927398e-01,  4.88050312e-01,  1.26904491e-02,\n",
       "         -1.83028802e-01,  1.17077686e-01,  2.19152510e-01,\n",
       "          4.37208593e-01, -3.94491971e-01, -6.06226087e-01,\n",
       "         -8.55026782e-01,  8.26562226e-01,  2.12591708e-01,\n",
       "         -8.22825581e-02,  3.06489259e-01, -7.77213991e-01,\n",
       "          5.82207024e-01,  1.23407622e-03, -8.08393657e-01,\n",
       "         -4.83796652e-03,  8.77140239e-02, -5.23269996e-02,\n",
       "          2.74096932e-02,  1.04022808e-02,  1.06515586e-01,\n",
       "         -4.86150235e-01, -1.43497691e-01,  3.43057126e-01,\n",
       "          7.64867961e-01, -3.04175466e-01,  2.50603841e-03,\n",
       "          1.85016736e-01, -2.38975286e-02,  1.42430678e-01,\n",
       "          4.29776385e-02,  3.89710099e-01, -2.88498193e-01,\n",
       "          4.13002521e-01, -6.08295143e-01, -1.82064846e-01,\n",
       "          2.59212852e-01,  3.80132347e-04,  2.86301404e-01,\n",
       "          3.10389519e-01,  2.26856008e-01,  9.46781635e-01,\n",
       "          4.35889155e-01, -6.09805109e-03, -8.26612115e-02,\n",
       "         -2.12691948e-01,  2.77293682e-01,  3.58016670e-01,\n",
       "         -1.79634914e-02, -7.58212581e-02, -4.33620244e-01,\n",
       "         -1.90939698e-02, -4.07566160e-01,  1.21403053e-01,\n",
       "         -4.97667372e-01,  1.25265062e-01, -3.13403219e-01,\n",
       "          4.51630801e-02,  4.25525337e-01, -2.82507598e-01,\n",
       "          2.83794582e-01,  4.29589838e-01, -9.68324952e-03,\n",
       "          2.72859484e-01,  5.22097528e-01,  5.64134074e-03,\n",
       "          1.45242866e-02,  3.65342721e-02, -7.72113800e-01,\n",
       "          2.08040595e-01,  3.82147372e-01,  7.49642432e-01,\n",
       "          3.04765970e-01, -2.12281182e-01, -2.04777837e-01,\n",
       "         -4.11131293e-01, -2.51595557e-01,  4.59036082e-01,\n",
       "          5.04252195e-01, -3.22512001e-01, -1.82018965e-03,\n",
       "          1.14487499e-01, -9.17421356e-02, -6.13689795e-02,\n",
       "         -2.87823141e-01, -6.48695111e-01, -2.27268055e-01,\n",
       "          3.43184769e-01, -4.98373359e-01,  7.12186322e-02,\n",
       "          1.42064586e-01, -9.47424546e-02,  1.13504022e-01,\n",
       "         -2.00776488e-01,  6.40414357e-01, -6.18282259e-02,\n",
       "         -9.21632290e-01, -5.96596777e-01,  8.45559165e-02,\n",
       "          2.43850634e-01, -2.95049489e-01, -1.64248630e-01,\n",
       "          7.17416108e-02,  1.62532162e-02, -7.00855479e-02,\n",
       "          8.12018514e-01,  2.45887399e-01,  1.85706064e-01,\n",
       "          3.28859538e-01, -2.32337520e-01, -7.54638612e-01,\n",
       "         -1.44890383e-01,  1.92581769e-02,  2.80639768e-01,\n",
       "         -8.85619819e-01,  2.42694497e-01,  5.57396173e-01,\n",
       "         -4.97671723e-01, -2.65495569e-01, -2.66552180e-01,\n",
       "         -1.00132018e-01, -1.40020087e-01, -3.77084285e-01,\n",
       "         -3.71289738e-02, -5.93224704e-01,  2.78400779e-01,\n",
       "         -1.59564212e-01, -3.34690779e-01, -3.67194831e-01,\n",
       "         -1.17486171e-01,  2.30464131e-01,  3.40060323e-01,\n",
       "          7.15862885e-02,  2.52730310e-01,  2.97424614e-01,\n",
       "          8.09659779e-01, -1.63418464e-02,  8.39881539e-01,\n",
       "         -8.25780258e-02]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[ 3.87924463e-02, -3.95189553e-01, -7.20027834e-02,\n",
       "          1.20065063e-01, -2.47502718e-02,  1.19087368e-01,\n",
       "          5.32330647e-02, -1.76824518e-02,  6.62891030e-01,\n",
       "          4.29192394e-01,  2.23115385e-02,  6.19717419e-01,\n",
       "         -8.91600922e-03, -1.57983340e-02, -9.68628749e-02,\n",
       "          1.06726475e-02,  7.46041298e-01, -9.89955425e-01,\n",
       "          9.99110341e-01,  8.81203562e-02, -1.00000000e+00,\n",
       "          3.04540575e-01,  9.99791622e-01, -2.51719505e-02,\n",
       "          2.83932447e-01, -5.22138298e-01, -3.10425553e-03,\n",
       "         -9.32226423e-03,  7.01342002e-02, -1.05137177e-01,\n",
       "          7.62742341e-01,  4.31496054e-01, -1.15241721e-01,\n",
       "         -1.27603576e-01,  1.79466531e-02, -9.92600694e-02,\n",
       "          7.27711022e-02,  9.98555303e-01, -3.74109238e-01,\n",
       "          5.88525057e-01, -8.04022048e-03, -1.15671992e-01,\n",
       "         -9.99820530e-01,  8.50208879e-01, -5.19041061e-01,\n",
       "         -5.81149578e-01,  9.76302147e-01, -6.92900836e-01,\n",
       "          1.00000000e+00,  1.41488850e-01, -2.79746950e-03,\n",
       "          9.98098016e-01,  4.63953651e-02, -3.53979141e-01,\n",
       "         -6.52307093e-01, -1.79574534e-01, -1.18566424e-01,\n",
       "          5.62774250e-03, -3.77281934e-01,  1.78678617e-01,\n",
       "         -3.74796502e-02, -1.62433729e-01,  1.77229702e-01,\n",
       "          8.03583860e-02, -1.53229814e-02, -2.81547546e-01,\n",
       "          5.79583272e-02, -8.27372372e-02,  9.89612639e-01,\n",
       "          7.14578032e-01, -2.94196635e-01,  2.46480666e-02,\n",
       "          3.94616753e-01,  3.13988298e-01,  1.46856776e-03,\n",
       "          2.90843435e-02,  8.37646425e-02, -4.08858269e-01,\n",
       "          1.00000000e+00, -1.47922605e-01,  7.20640570e-02,\n",
       "          4.01506154e-03,  1.11134656e-01, -6.70547038e-02,\n",
       "          6.46447577e-03,  2.93893009e-01, -1.81405902e-01,\n",
       "          4.19005007e-01, -3.17546457e-01,  1.95840463e-01,\n",
       "         -2.51068175e-01, -6.10292494e-01,  1.43378731e-02,\n",
       "          2.96431687e-02,  1.00158788e-01,  1.13538280e-02,\n",
       "          1.22672215e-01,  1.27240017e-01,  7.44439185e-01,\n",
       "          3.39451283e-01,  4.47797664e-02,  3.71236242e-02,\n",
       "          3.55706038e-03,  5.91527760e-01, -1.11327171e-01,\n",
       "         -4.47373986e-01,  6.54933006e-02,  8.71211886e-02,\n",
       "          7.27828741e-01,  3.25574815e-01,  1.07421994e-01,\n",
       "          1.22773774e-01, -1.79904833e-01, -5.92101812e-01,\n",
       "          1.29241229e-03, -2.35582255e-02,  1.04111321e-01,\n",
       "         -1.52441608e-02,  1.97334781e-01,  1.90131009e-01,\n",
       "          5.45911431e-01,  1.70333125e-03, -2.77039111e-01,\n",
       "          3.46021168e-02, -5.45631766e-01, -3.28299962e-02,\n",
       "         -7.32511282e-01,  5.04640043e-01, -7.43674412e-02,\n",
       "         -7.94741586e-02,  1.66061558e-02,  5.87933004e-01,\n",
       "         -2.04076208e-02, -1.00000000e+00, -5.10149822e-02,\n",
       "          2.02067196e-04, -1.51727542e-01,  1.52982786e-01,\n",
       "          1.56107217e-01,  1.94500893e-01,  3.95334996e-02,\n",
       "          2.95781791e-02,  1.68800924e-03, -3.82121146e-01,\n",
       "         -2.49823369e-02, -4.21933942e-02,  9.84482050e-01,\n",
       "         -4.04035673e-03, -1.52626559e-01, -2.01028720e-01,\n",
       "         -4.58347537e-02, -2.94528808e-02,  1.37182446e-02,\n",
       "         -7.62580410e-02, -1.29813895e-01,  8.67220938e-01,\n",
       "          9.97783422e-01,  5.20404056e-02,  6.46859407e-05,\n",
       "          3.32593098e-02, -8.16107571e-01,  4.10343230e-01,\n",
       "         -1.81748327e-02,  7.65681744e-01,  2.77335912e-01,\n",
       "          9.99544442e-01,  1.89668946e-02, -2.61142049e-02,\n",
       "          2.15106793e-02,  6.96405172e-01, -1.57901302e-01,\n",
       "          8.24696478e-03, -1.21053554e-01, -1.07423700e-02,\n",
       "         -3.76755372e-02, -1.98374286e-01,  4.49152254e-02,\n",
       "         -9.10285711e-02,  9.98026848e-01,  2.21675977e-01,\n",
       "         -7.09456727e-02,  2.98212022e-01, -3.95134866e-01,\n",
       "          7.22642848e-03,  1.72389951e-02,  1.24119140e-01,\n",
       "         -7.12283403e-02,  9.54640746e-01, -9.14477408e-02,\n",
       "         -4.88889724e-01, -8.88394192e-03,  3.43467206e-01,\n",
       "          7.46602535e-01, -7.75147825e-02,  6.54107332e-03,\n",
       "          3.29649970e-02,  1.00000000e+00,  1.00000000e+00,\n",
       "         -3.11318804e-02, -6.22137487e-02,  3.86667997e-01,\n",
       "         -6.62936568e-02, -6.22296892e-03, -5.15101328e-02,\n",
       "         -5.86002529e-01,  2.46097520e-01,  7.52349973e-01,\n",
       "          4.68885452e-01, -1.26805991e-01, -3.55780199e-02,\n",
       "          6.82593649e-03, -4.07271422e-02,  7.14112580e-01,\n",
       "          1.67647764e-01,  1.69269189e-01, -1.07502360e-02,\n",
       "         -1.29336625e-01,  3.65580916e-01,  6.97646081e-01,\n",
       "          6.40212119e-01, -3.57769042e-01,  3.27251889e-02,\n",
       "          6.71459973e-01,  5.23124516e-01,  8.54844972e-02,\n",
       "          9.97759342e-01, -9.99998033e-01, -2.20651776e-02,\n",
       "         -2.88218241e-02, -2.61915743e-01,  8.37095797e-01,\n",
       "         -2.83913583e-01,  6.53570220e-02, -5.62683403e-01,\n",
       "          2.56890416e-01,  7.43386626e-01, -1.41123578e-01,\n",
       "          4.19753164e-01,  2.54463345e-01,  2.87308216e-01,\n",
       "          6.62898198e-02,  1.62408873e-01, -4.84479129e-01,\n",
       "          3.43440622e-01, -8.58144555e-03,  2.36108643e-03,\n",
       "          1.46924764e-01,  4.49515790e-01, -3.49868485e-03,\n",
       "          1.70225501e-01, -3.97786126e-02,  1.87625811e-01,\n",
       "         -8.07998180e-02, -8.56238306e-01,  7.31948912e-01,\n",
       "         -2.87006889e-02]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[ 2.7604802e-02, -2.4423745e-01,  5.5571988e-02, -5.7771329e-05,\n",
       "          4.5124665e-04,  9.6522039e-01, -9.9977875e-01,  5.3796200e-03,\n",
       "         -9.4736242e-01,  4.9211155e-04, -5.1317817e-01,  9.9921787e-01,\n",
       "          3.2470596e-01, -1.3629753e-02, -3.1170319e-04,  9.9686247e-01,\n",
       "         -1.0793629e-03, -9.9680924e-01,  2.5785139e-01,  7.3126387e-03,\n",
       "          6.0786381e-02, -5.2328724e-03,  3.5402182e-01,  8.1450641e-02,\n",
       "          1.5826141e-02,  1.1419624e-03,  7.8195650e-03, -4.0696594e-01,\n",
       "         -6.4892601e-04,  1.8539598e-03,  1.2359636e-01,  9.8042315e-01,\n",
       "         -6.2007911e-04,  6.6754496e-01, -1.5102798e-01, -4.7451192e-01,\n",
       "          7.4287974e-03, -9.9999452e-01,  6.7635342e-03,  1.5041925e-04,\n",
       "          1.7164297e-03, -9.9744838e-01,  2.5750976e-04, -5.7115648e-03,\n",
       "          5.0519985e-01,  3.8628510e-01,  6.5985811e-04,  9.9100268e-01,\n",
       "         -9.9903786e-01,  8.4999222e-01, -6.9903064e-04,  8.6279285e-01,\n",
       "         -8.9572757e-01, -2.9815212e-01, -5.4964656e-01,  2.9245463e-01,\n",
       "         -9.9150658e-01,  9.9109465e-01,  1.8197593e-03, -2.2443137e-01,\n",
       "         -5.0924762e-05,  7.7819373e-03,  8.1315839e-01, -1.3994848e-03,\n",
       "         -1.1382449e-03, -9.9982697e-01, -5.1214409e-01,  1.2752469e-02,\n",
       "         -6.2178981e-02,  3.8819894e-01, -1.3861578e-03,  4.8366189e-02,\n",
       "          4.5481778e-04, -2.3173655e-03, -5.9619224e-01,  8.9365896e-03,\n",
       "         -9.7624294e-04,  2.5648128e-03, -9.9940794e-03, -4.9083605e-01,\n",
       "         -2.3095810e-03, -1.1984761e-03,  3.7763852e-01, -3.9554262e-04,\n",
       "         -5.9656432e-04, -4.4913307e-02, -9.9952441e-01, -7.6174989e-02,\n",
       "         -2.7927691e-03, -8.1356132e-01,  1.0447878e-01, -7.8709126e-03,\n",
       "         -1.1629089e-03, -3.8077322e-01,  5.7371211e-01,  4.1871178e-01,\n",
       "          9.9999833e-01, -6.4361189e-04,  1.1824220e-02, -7.4345255e-03,\n",
       "         -9.9157155e-01, -8.1243820e-04, -4.1953698e-01,  4.5744586e-04,\n",
       "          9.8658083e-03, -9.7599715e-01, -1.5180501e-01, -3.3990663e-01,\n",
       "         -6.0780258e-03, -5.6338561e-04,  1.9850552e-01, -9.6378511e-01,\n",
       "         -1.9697741e-01, -6.2575229e-03,  4.0076274e-04,  4.4147359e-04,\n",
       "          9.9988341e-01, -8.4764886e-01,  2.7476013e-01,  3.0011612e-01,\n",
       "         -4.2285365e-03,  5.3305347e-02,  3.0483911e-04,  1.5046139e-02,\n",
       "          6.9109426e-04,  6.6792613e-01, -5.1654583e-01, -2.2144316e-01,\n",
       "         -9.9678451e-01, -1.8976044e-03,  1.4811326e-02, -2.6810032e-03,\n",
       "          3.1447411e-04,  8.8875681e-01,  1.0718337e-02,  7.9043425e-04,\n",
       "         -9.9913079e-01,  5.3430372e-01, -1.2033717e-02, -4.5059709e-04,\n",
       "          5.6168985e-01,  6.2769498e-03, -7.2351290e-04,  1.3446759e-03,\n",
       "         -7.0246775e-04,  8.5332185e-01, -5.4151644e-03,  5.8872707e-04,\n",
       "         -2.8108775e-03, -2.4272589e-01,  7.5268797e-03,  1.4914843e-01,\n",
       "          3.1823042e-01, -8.2378723e-02,  1.3813956e-03, -2.9771363e-02,\n",
       "         -2.0812922e-03, -1.4175970e-02, -8.4099025e-01,  8.5887173e-03,\n",
       "          3.6591489e-04,  5.6265873e-01, -7.9040921e-01,  8.2351797e-04,\n",
       "          2.5153039e-03, -1.0000000e+00, -6.9380027e-01,  3.0682105e-01,\n",
       "         -1.3261236e-01, -2.8336585e-01, -3.5540471e-03, -4.1082430e-01,\n",
       "          6.5737828e-03,  1.1163590e-03,  9.3925393e-01,  3.4112635e-01,\n",
       "          6.8973625e-01, -1.4407710e-03, -8.0585551e-01, -2.3849380e-01,\n",
       "         -1.0000000e+00,  4.8904896e-01,  3.5724833e-03,  7.6513851e-01,\n",
       "          6.3540568e-03,  2.6359584e-02, -6.1355310e-04,  9.9279994e-01,\n",
       "          3.0153524e-03,  9.9286616e-01, -3.4762133e-04, -1.7875851e-03,\n",
       "          5.1996231e-01,  1.9510477e-03, -4.8755604e-01,  8.0974074e-05,\n",
       "          2.4107464e-01,  1.8314245e-03, -3.2605451e-01,  2.6595308e-03,\n",
       "          1.0410133e-03, -8.1874275e-01, -4.8517190e-02, -7.7621037e-01,\n",
       "          9.8603320e-01,  5.1801144e-03,  9.9690002e-01,  1.0000000e+00,\n",
       "         -4.8494639e-04,  7.0371805e-03, -1.0901774e-02,  3.6264336e-01,\n",
       "          9.5530438e-01,  6.7039752e-01, -7.8705007e-01, -3.8280685e-03,\n",
       "          2.2419060e-02,  4.4064510e-01, -8.4129557e-02,  9.9346590e-01,\n",
       "         -4.8504636e-01,  1.2595912e-03, -5.1184601e-01, -1.8907938e-04,\n",
       "         -6.2081516e-01, -3.6441004e-01, -1.7830542e-03, -6.8324113e-01,\n",
       "          1.2068384e-02, -3.8723184e-03, -2.0891749e-03,  1.0013326e-01,\n",
       "          1.0000000e+00, -3.0881382e-04,  9.7107142e-01,  1.0000000e+00,\n",
       "         -5.7400519e-04,  6.0169387e-01,  1.4034772e-03, -7.5492222e-05,\n",
       "         -1.5900560e-01,  1.6790748e-01, -6.9881533e-04, -3.1856200e-01,\n",
       "          8.4902269e-01, -1.6123764e-04, -5.6259864e-04,  4.5173061e-01,\n",
       "         -5.5997014e-05,  1.3025604e-02, -3.7282544e-01,  2.6188872e-04,\n",
       "         -2.9876065e-03,  5.3994562e-02, -9.5752645e-01,  8.4172022e-03]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , Y = next(train_generator(1))\n",
    "\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(batch_size) :\n",
    "    \n",
    "    start = 0 \n",
    "    train_data_size = len(validation_data)\n",
    "\n",
    "    while True : \n",
    "        \n",
    "        char_level_tokens = []\n",
    "        word_level_tokens = []\n",
    "\n",
    "        sgns_embeddings = []\n",
    "        char_embeddings = []\n",
    "        electra_embeddings = []\n",
    "        \n",
    "        end = start + batch_size\n",
    "        \n",
    "        if end > train_data_size : \n",
    "            end = train_data_size \n",
    "        elif end == train_data_size : \n",
    "            start = 0\n",
    "            end = start + batch_size\n",
    "        else : \n",
    "            pass\n",
    "        \n",
    "        curr_batch = validation_data[start:end]\n",
    "        \n",
    "        for sample in curr_batch : \n",
    "            \n",
    "            gloss = sample['gloss']\n",
    "            char_level_tokens.append(tokenize(gloss, char_tokenizer, tokenize_level=\"CHAR\", pad=True, target_len=char_seq_len))\n",
    "            word_level_tokens.append(tokenize(gloss, char_tokenizer, tokenize_level=\"WORD\", pad=True, target_len=word_seq_len))\n",
    "            \n",
    "            sgns_embeddings.append(sample['sgns'])\n",
    "            char_embeddings.append(sample['char'])\n",
    "            electra_embeddings.append(sample['electra'])\n",
    "            \n",
    "        yield (np.array(char_level_tokens), np.array(word_level_tokens)),\\\n",
    "        (np.array(sgns_embeddings), np.array(char_embeddings), np.array(electra_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples :  6375\n",
      "24/24 [==============================] - 22s 920ms/step - loss: 4.2393 - dense_45_loss: 1.1200 - dense_46_loss: 0.2430 - dense_47_loss: 2.8763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.239328861236572,\n",
       " 1.1200318336486816,\n",
       " 0.24299781024456024,\n",
       " 2.8762998580932617]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X = []\n",
    "val_Y = []\n",
    "\n",
    "\n",
    "for sample in validation_data : \n",
    "    \n",
    "    char_level_tokens = []\n",
    "    word_level_tokens = []\n",
    "\n",
    "    sgns_embeddings = []\n",
    "    char_embeddings = []\n",
    "    electra_embeddings = []\n",
    "    \n",
    "    gloss = sample['gloss']\n",
    "    \n",
    "    char_level_tokens.append(tokenize(gloss, char_tokenizer, tokenize_level=\"CHAR\", pad=True, target_len=char_seq_len))\n",
    "    word_level_tokens.append(tokenize(gloss, char_tokenizer, tokenize_level=\"WORD\", pad=True, target_len=word_seq_len))\n",
    "\n",
    "    sgns_embeddings.append(sample['sgns'])\n",
    "    char_embeddings.append(sample['char'])\n",
    "    electra_embeddings.append(sample['electra'])\n",
    "    \n",
    "    val_X.append((np.array(char_level_tokens), np.array(word_level_tokens)))\n",
    "    val_Y.append((np.array(sgns_embeddings), np.array(char_embeddings), np.array(electra_embeddings)))\n",
    "\n",
    "    # val_X.append(tuple(X))\n",
    "    # val_Y.append(tuple(Y))\n",
    "    \n",
    "    \n",
    "print(\"Number of validation samples : \", len(val_X))\n",
    "assert len(val_X)==len(val_Y)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model.evaluate(val_generator(batch_size), batch_size=batch_size, steps=int(len(validation_data)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 2.9938 - dense_45_loss: 2.2001 - dense_46_loss: 0.2920 - dense_47_loss: 0.5017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.993809938430786, 2.200071096420288, 0.2920176088809967, 0.5017211437225342]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([val_X[0]], [val_Y[0]], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 14:37:23.633559: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-12-06 14:37:30.605374: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2021-12-06 14:37:31.235069: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n",
      "2021-12-06 14:37:31.859863: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 805306368 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vanilla_lstm/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/vanilla_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
